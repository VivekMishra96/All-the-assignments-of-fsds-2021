{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 03 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1ac3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c03a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975680c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d645f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59317265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446a69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0528d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "1.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "2.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "3.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "4.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "5.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256c5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
